{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem Set 5\n",
    "Designed by Ben Usman, Sarah Adel Bargal, and Brian Kulis, with help from Kun He and Kate Saenko.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "\n",
    "1. implementation of RNN and LSTM\n",
    "2. training your implemented LSTM\n",
    "3. deriving multi-modal Restricted Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: RNN Example\n",
    "\n",
    "(10 points)\n",
    "\n",
    "In this example we train an RNN to infer the parameters of a simple dynamical system.\n",
    "First, we simulate a dynamical system with known parameters (random numbers), and use it to generate outputs. Then, we train an RNN on the generated datapoints, attempting to infer the original parameters.\n",
    "You are expected to run and study the provided code, which will be helpful for the second part (implementing LSTM).\n",
    "\n",
    "1. We define a discretized [dynamical system](https://en.wikipedia.org/wiki/Dynamical_system).\n",
    "At each discrete time $t$, the system observes input $x_t$. The system maintains some \"state\" $h_t$, which will be updated over time.\n",
    "Specifically, the states are updated by the following rule: $h_t = \\max(0, 1-(Wx_t+h_{t-1}))$, where $W$ is a parameter matrix.\n",
    "2. In this example, the input data $\\{x_t\\}$ is randomly generated, and the weight matrix $W$ is also drawn randomly. The system starts from an initial hidden state $h_0$, and runs for $t=1,\\ldots,T$.\n",
    "3. Given the sequence of states $\\{h_1,\\ldots,h_T\\}$, we would like to infer the weights $W$.\n",
    "\n",
    "To start with, the following code segment generates and displays the data.\n",
    "Refer to `data_generator.py` for details of data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable name, shape, min, max: \n",
      "h0 (10,) -1.64432771713 0.888951912163\n",
      "w (10, 10) -2.1527462226 2.62799071843\n",
      "x (100, 10) -3.25659366864 3.65327854995\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8854cea160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we present sample data generated \n",
    "import numpy as np\n",
    "import data_generator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(h_0, w), x, h = data_generator._build_rnn_testdata_matrix()\n",
    "\n",
    "print('variable name, shape, min, max: ')\n",
    "for v, name in zip([h_0, w, x], ['h0', 'w', 'x']):\n",
    "    print(name, v.shape, np.min(v), np.max(v))\n",
    "norm_x_t = np.sum(x**2, axis=1)\n",
    "plt.title('||x(t)||')\n",
    "plt.plot(np.arange(norm_x_t.shape[0]), norm_x_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We model the dynamical system with an RNN. We would like the state generated by the RNN to match the actual observed, and we use L2 loss for this purpose.\n",
    "This RNN is a *regression* model since it outputs real values.\n",
    "\n",
    "Below, `build_rnn_regression_model()` gives the model definition, and `train_rnn_with_noise()` generates a batch of data and then runs training on it. \n",
    "We corrupt the data generation process with noise, and let the dimensionality of the state $h$ be a free parameter. Therefore, the training function takes two input arguments: `noise_level` and `n_hidden_dim`.   You will later see how varying them affects reconstruction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/share/pkg/python/3.6.0/install/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/share/pkg/python/3.6.0/install/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: /share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyInstance_Type\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdlopenflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_default_dlopen_flags\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTLD_GLOBAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdlopenflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_default_dlopen_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.0/install/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/python/3.6.0/install/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: /share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyInstance_Type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3d70ed3aebbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/share/pkg/python/3.6.0/install/lib/python3.6/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/share/pkg/python/3.6.0/install/lib/python3.6/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: /share/pkg/tensorflow/r1.0/install/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: PyInstance_Type\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "\n",
    "#############################################################################\n",
    "# RNN model graph\n",
    "def build_rnn_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs to the dynamical system\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            X = tf.placeholder(tf.float32,\n",
    "                               [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "            # observed state from the dynamical system\n",
    "            y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "\n",
    "            with tf.variable_scope('weights'):\n",
    "                # weight matrix\n",
    "                w = tf.get_variable('w', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "                # initial state\n",
    "                h_0 = tf.get_variable('h_0', [shape['n_hidden_dim']])\n",
    "\n",
    "            # for t = 1 to T, update state \n",
    "            h_t = h_0\n",
    "            for t in range(shape['n_steps_per_batch']):\n",
    "                x_t = X[:, t, :]\n",
    "                h_t = tf.maximum(0.0, 1 - (tf.matmul(x_t, w) + h_t))\n",
    "                \n",
    "            # loss: L2\n",
    "            loss = tf.nn.l2_loss(h_t - y, name='loss')\n",
    "            train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "            summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)\n",
    "        \n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w': w, 'h_0': h_0}, 'graph': g}\n",
    "\n",
    "#############################################################################\n",
    "# Main train loop for an RNN regression model\n",
    "# \n",
    "# This takes synthetic data generated by data_generator.build_dataset()\n",
    "# the weight matrix W is then inferred with back-prop \n",
    "def train_rnn_with_noise(noise_level, n_hidden_dim):\n",
    "    # generate data\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    rnn_dataset = data_generator.build_dataset('rnn', noise=noise_level, **shapes)\n",
    "    (h0_true, w_true), batched_data = rnn_dataset  # \"true\" weights\n",
    "    # build RNN model\n",
    "    model = build_rnn_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/rnn_demo'  # if on Windows\n",
    "    logdir = './tensorboard/rnn_demo'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    # If you want to see the plots, run tensorboard:\n",
    "    # $ tensorboard --logdir=[your_logdir]\n",
    "    #\n",
    "    # If you use SCC, you can forward the 6006 port from the cluster \n",
    "    # to your local machine via:\n",
    "    # $ ssh [SCC_cluster_name] -L 6006:localhost:6006\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(50):\n",
    "            loss_val, w_dist, h0_dist, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                # run training step\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss'],\n",
    "                              model['weights']['w'], model['weights']['h_0']]\n",
    "                _, summ, loss_val, w_val, h0_val = sess.run(to_compute, train_feed_dict)\n",
    "                \n",
    "                # compute reconstruction error\n",
    "                w_err = np.linalg.norm(w_true-w_val)\n",
    "                h0_err = np.linalg.norm(h0_true-h0_val)\n",
    "                \n",
    "                # for tensorboard\n",
    "                sum_writer.add_summary(summ, global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"w_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=w_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"h_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=h0_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.flush()\n",
    "                \n",
    "            print('epoch %d, loss=%g, w_err=%g'%(epoch_i, loss_val, w_err))\n",
    "            if global_step > 200: \n",
    "                break  # just train for 200 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.\n",
    "Now test the RNN wth varying noise levels and hidden dimensionalities.\n",
    "- For each combination of `n_hidden_dim` and `noise_level`, report the reconstruction error (`w_err`).\n",
    "- Describe how the hidden dimentionality and the noise level influence reconstruction quality. And briefly explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "./tensorboard/rnn_demo/hidden=10_noise=0.00_27-Apr-14-49-22\n",
      "epoch 0, loss=22.2892, w_err=2.07589\n",
      "epoch 1, loss=3.94623, w_err=0.828624\n",
      "epoch 2, loss=0.523516, w_err=0.288374\n",
      "epoch 3, loss=0.0672878, w_err=0.106683\n",
      "epoch 4, loss=0.0168001, w_err=0.0638383\n",
      "epoch 5, loss=0.0273002, w_err=0.0454453\n",
      "epoch 6, loss=0.0415354, w_err=0.0359064\n",
      "epoch 7, loss=0.0409958, w_err=0.0353572\n",
      "10 0.1\n",
      "./tensorboard/rnn_demo/hidden=10_noise=0.10_27-Apr-14-49-48\n",
      "epoch 0, loss=19.2666, w_err=2.3689\n",
      "epoch 1, loss=4.08486, w_err=1.1221\n",
      "epoch 2, loss=1.79544, w_err=0.469611\n",
      "epoch 3, loss=2.00569, w_err=0.432777\n",
      "epoch 4, loss=2.60183, w_err=0.509764\n",
      "epoch 5, loss=3.2211, w_err=0.55213\n",
      "epoch 6, loss=3.45477, w_err=0.605851\n",
      "epoch 7, loss=3.60187, w_err=0.646565\n",
      "10 0.5\n",
      "./tensorboard/rnn_demo/hidden=10_noise=0.50_27-Apr-14-50-16\n",
      "epoch 0, loss=103.991, w_err=3.62989\n",
      "epoch 1, loss=49.2646, w_err=2.16078\n",
      "epoch 2, loss=41.6292, w_err=2.21522\n",
      "epoch 3, loss=45.0914, w_err=2.15057\n",
      "epoch 4, loss=46.1693, w_err=2.18158\n",
      "epoch 5, loss=48.0593, w_err=2.18846\n",
      "epoch 6, loss=47.794, w_err=2.24234\n",
      "epoch 7, loss=50.8351, w_err=2.24671\n",
      "100 0\n",
      "./tensorboard/rnn_demo/hidden=100_noise=0.00_27-Apr-14-50-42\n",
      "epoch 0, loss=354.506, w_err=7.27621\n",
      "epoch 1, loss=38.9815, w_err=2.5378\n",
      "epoch 2, loss=5.27353, w_err=0.896175\n",
      "epoch 3, loss=0.863619, w_err=0.35392\n",
      "epoch 4, loss=0.299248, w_err=0.202655\n",
      "epoch 5, loss=0.166312, w_err=0.148129\n",
      "epoch 6, loss=0.146062, w_err=0.137797\n",
      "epoch 7, loss=0.397004, w_err=0.156457\n",
      "100 0.1\n",
      "./tensorboard/rnn_demo/hidden=100_noise=0.10_27-Apr-14-51-11\n",
      "epoch 0, loss=351.975, w_err=7.86189\n",
      "epoch 1, loss=57.4349, w_err=2.88514\n",
      "epoch 2, loss=23.5352, w_err=1.72051\n",
      "epoch 3, loss=21.9874, w_err=1.56497\n",
      "epoch 4, loss=24.0795, w_err=1.63196\n",
      "epoch 5, loss=26.7902, w_err=1.74598\n",
      "epoch 6, loss=28.8105, w_err=1.84994\n",
      "epoch 7, loss=31.0814, w_err=1.9716\n",
      "100 0.5\n",
      "./tensorboard/rnn_demo/hidden=100_noise=0.50_27-Apr-14-51-39\n",
      "epoch 0, loss=676.647, w_err=9.12241\n",
      "epoch 1, loss=430.878, w_err=5.91862\n",
      "epoch 2, loss=415.492, w_err=5.84496\n",
      "epoch 3, loss=425.974, w_err=5.80737\n",
      "epoch 4, loss=442.894, w_err=5.87935\n",
      "epoch 5, loss=442.49, w_err=6.00851\n",
      "epoch 6, loss=456.271, w_err=6.05953\n",
      "epoch 7, loss=466.924, w_err=6.12898\n",
      "1000 0\n",
      "./tensorboard/rnn_demo/hidden=1000_noise=0.00_27-Apr-14-52-09\n",
      "epoch 0, loss=5226.8, w_err=30.2236\n",
      "epoch 1, loss=449.641, w_err=8.54591\n",
      "epoch 2, loss=51.1202, w_err=2.87371\n",
      "epoch 3, loss=7.54842, w_err=1.10221\n",
      "epoch 4, loss=1.6876, w_err=0.511205\n",
      "epoch 5, loss=0.730931, w_err=0.323852\n",
      "epoch 6, loss=0.533125, w_err=0.257256\n",
      "epoch 7, loss=0.379233, w_err=0.266424\n",
      "1000 0.1\n",
      "./tensorboard/rnn_demo/hidden=1000_noise=0.10_27-Apr-14-52-44\n",
      "epoch 0, loss=6119.31, w_err=31.4881\n",
      "epoch 1, loss=653.919, w_err=9.45892\n",
      "epoch 2, loss=254.345, w_err=5.14328\n",
      "epoch 3, loss=227.282, w_err=4.411\n",
      "epoch 4, loss=245.564, w_err=4.59022\n",
      "epoch 5, loss=270.622, w_err=4.87009\n",
      "epoch 6, loss=295.175, w_err=5.18876\n",
      "epoch 7, loss=322.147, w_err=5.50355\n",
      "1000 0.5\n",
      "./tensorboard/rnn_demo/hidden=1000_noise=0.50_27-Apr-14-53-17\n",
      "epoch 0, loss=8878.29, w_err=36.3457\n",
      "epoch 1, loss=3975.03, w_err=18.7612\n",
      "epoch 2, loss=3730.2, w_err=18.6553\n",
      "epoch 3, loss=3836.55, w_err=18.7331\n",
      "epoch 4, loss=3944.94, w_err=18.9858\n",
      "epoch 5, loss=4066.91, w_err=19.2373\n",
      "epoch 6, loss=4159.33, w_err=19.4264\n",
      "epoch 7, loss=4245.07, w_err=19.6223\n"
     ]
    }
   ],
   "source": [
    "# Experimenting with different data noise levels and hidden dimentionalities\n",
    "# We are lucky to know the true hidden dimentionality in our simultaion\n",
    "for n_hidden_dim in [10, 100, 1000]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print(n_hidden_dim, noise_level)\n",
    "        train_rnn_with_noise(noise_level, n_hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: LSTM implementation\n",
    "\n",
    "(40 points)\n",
    "\n",
    "Now let's attempt to recover the weights in dynamical system simulated with an LSTM.  Although LSTMs are already implemented in TensorFlow ([tutorial here](https://www.tensorflow.org/tutorials/recurrent)) ([source here](https://github.com/tensorflow/tensorflow/blob/efe5376f3dec8fcc2bf3299a4ff4df6ad3591c88/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L264)), you will be implementing a simple LSTM from scratch using Tensorflow in this part. \n",
    "\n",
    "\n",
    "### Q2. Implement an LSTM \n",
    "Implement an LSTM model in an analogous way,  in functions `build_lstm_regression_model()` and `train_lstm_with_noise()` below.\n",
    "The RNN regression implementation above should give you some ideas.\n",
    "We already provided an LSTM version of the dynamical system generator in the dataset generator code.\n",
    "\n",
    "- Specifically, you can follow and implement Eq. 1-6 from [this link](http://deeplearning.net/tutorial/lstm.html) in `build_lstm_regression_model()`. \n",
    "You may simplify your code by concatenating $x$ and $h$.\n",
    "- Afterwards, implement `train_lstm_with_noise()` to train the LSTM and recover the parameters. Compute the reconstruction errors for $W_c$ and $U_c$, which are the parameters used in Eq. 2 in the link.\n",
    "- For each combination of hidden dimension and noise level, report the reconstruction error (`w_err`, `u_err`) you get from the LSTM.\n",
    "\n",
    "Note:\n",
    "- The weights might not get reconstructed correctly in the LSTM case even without noise. (Why?)\n",
    "Nevertheless, the loss should decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_lstm_regression_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_hidden_dim, n_input_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # inputs\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                           [None, shape['n_steps_per_batch'], shape['n_input_dim']])\n",
    "        # observed outputs\n",
    "        y = tf.placeholder(tf.float32, [None, shape['n_hidden_dim']])\n",
    "        \n",
    "        #################################################################\n",
    "        ####################  PUT YOUR CODE HERE   ######################\n",
    "        # define the parameters in the LSTM (scope: weights)\n",
    "        # and carry out the computation according to Eq. 1-6 in the link\n",
    "        with tf.variable_scope('weights'):\n",
    "           \n",
    "            \n",
    "            h_0 = tf.get_variable('h_9', [1, shape['n_hidden_dim']])\n",
    "            c_0 = tf.get_variable('c_0', [1, shape['n_hidden_dim']])\n",
    "            w_i = tf.get_variable('w_i', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_c = tf.get_variable('w_c', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_f = tf.get_variable('w_f', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            w_o = tf.get_variable('w_o', [shape['n_input_dim'], shape['n_hidden_dim']])\n",
    "            u_i = tf.get_variable('u_i', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_c = tf.get_variable('u_c', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_f = tf.get_variable('u_f', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_o = tf.get_variable('u_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            v_o = tf.get_variable('v_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            \n",
    "        h_prev = h_0\n",
    "        c_prev = c_0\n",
    "        x_t = X[:, 0, :]\n",
    "        i_t = tf.sigmoid(tf.matmul(x_t, w_i) + tf.reshape(tf.matmul(h_prev, u_i), [shape['n_hidden_dim']])) #(batch, hidden)\n",
    "        c_bar_t = tf.tanh(tf.matmul(x_t, w_c) + tf.reshape(tf.matmul(h_prev, u_c), [shape['n_hidden_dim']]))#(batch, hidden)\n",
    "        f_t = tf.sigmoid(tf.matmul(x_t, w_f) + tf.reshape(tf.matmul(h_prev, u_f), [shape['n_hidden_dim']]))#(batch, hidden)\n",
    "        c_t = i_t * c_bar_t + f_t * c_prev #(batch, hidden)\n",
    "        o_t = tf.sigmoid(tf.matmul(x_t, w_o) + tf.reshape(tf.matmul(h_prev, u_o), [shape['n_hidden_dim']]) + \\\n",
    "                         tf.matmul(c_t, v_o)) #(batch, hidden)\n",
    "        h_t = o_t * tf.tanh(c_t)  #(batch, hidden)\n",
    "        h_prev = h_t  #(batch, hidden)\n",
    "        c_prev = c_t  #(batch, hidden)\n",
    "        for t in range(1, shape['n_steps_per_batch']):\n",
    "            x_t = X[:, t, :] \n",
    "            i_t = tf.sigmoid(tf.matmul(x_t, w_i) + tf.matmul(h_prev, u_i))  #(batch, hidden)\n",
    "            c_bar_t = tf.tanh(tf.matmul(x_t, w_c) + tf.matmul(h_prev, u_c))  #(batch, hidden)\n",
    "            f_t = tf.sigmoid(tf.matmul(x_t, w_f) + tf.matmul(h_prev, u_f))  #(batch, hidden)\n",
    "            c_t = i_t * c_bar_t + c_prev * f_t  #(batch, hidden)\n",
    "            o_t = tf.sigmoid(tf.matmul(x_t, w_o) + tf.matmul(h_prev, u_o) + tf.matmul(c_t, v_o))  #(batch, hidden)\n",
    "            h_t = o_t * tf.tanh(c_t) #(batch, hidden)\n",
    "            h_prev = h_t  #(batch, hidden)\n",
    "            c_prev = c_t  #(batch, hidden)\n",
    "                \n",
    "        output = h_t  # put your result in variable 'output'\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        # loss and train_op\n",
    "        loss = tf.nn.l2_loss(output - y, name='loss')\n",
    "        train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "        summ = tf.summary.scalar('loss_sum_%dd' % shape['n_hidden_dim'], loss)\n",
    "\n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'weights': {'w_c': w_c, 'u_c': u_c}, \n",
    "            'graph': g}\n",
    "    \n",
    "    \n",
    "def train_lstm_with_noise(noise_level, n_hidden_dim):\n",
    "    # generate data and random weights\n",
    "    shapes = dict(n_hidden_dim=n_hidden_dim, n_input_dim=15, n_steps_per_batch=100)\n",
    "    weights, batched_data = data_generator.build_dataset('lstm', noise=noise_level, **shapes)\n",
    "    w_c, u_c = weights[3], weights[7]  # the \"true\" weights to recover: Wc & Uc (in Eq.2)\n",
    "    \n",
    "    # this is the function you implemented\n",
    "    model = build_lstm_regression_model(shapes)\n",
    "    \n",
    "    #logdir = './tensorboard/lstm_demo'  # if on Windows\n",
    "    logdir = './tensorboard/lstm_demo'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d_noise=%.2f' % (n_hidden_dim, noise_level)\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    \n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(10):  # 10 epochs by default, feel free to change\n",
    "            loss_val, w_err, u_err, iter_i = None, None, None, None\n",
    "            for iter_i, data_batch in enumerate(batched_data):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                ###############################################################\n",
    "                ###################   PUT YOUR CODE HERE   ####################\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss'],\n",
    "                              model['weights']['w_c'], model['weights']['u_c']]\n",
    "                _, summ, loss_val, w_val, u_val = sess.run(to_compute, train_feed_dict)\n",
    "            \n",
    "                \n",
    "                w_err = np.linalg.norm(w_c - w_val)  # compute them and report\n",
    "                u_err = np.linalg.norm(u_c - u_val)\n",
    "                # [optional] you can also do early stopping, e.g. 300 iterations\n",
    "                # if global_step > 300:\n",
    "                #    break\n",
    "                # use sum_writer for tensorboard, see train_rnn_with_noise()\n",
    "                sum_writer.add_summary(summ, global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"w_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=w_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.add_summary(tf.Summary(value=[\n",
    "                    tf.Summary.Value(tag=\"h_true_dist_%dd\" % n_hidden_dim,\n",
    "                                     simple_value=u_err),\n",
    "                ]), global_step)\n",
    "                sum_writer.flush()\n",
    "                ###############################################################\n",
    "            print('epoch %d, loss=%g, w_err=%g'%(epoch_i, loss_val, w_err))\n",
    "            if global_step > 200: \n",
    "                break  # just train for 200 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "./tensorboard/lstm_demo/hidden=10_noise=0.00_27-Apr-15-03-45\n",
      "epoch 0, loss=10.1601, w_err=8.30803\n",
      "epoch 1, loss=7.91528, w_err=7.21294\n",
      "epoch 2, loss=5.43464, w_err=6.26387\n",
      "epoch 3, loss=5.57354, w_err=5.65392\n",
      "epoch 4, loss=4.79182, w_err=5.58973\n",
      "epoch 5, loss=5.24243, w_err=5.78585\n",
      "epoch 6, loss=4.59838, w_err=6.13913\n",
      "epoch 7, loss=4.10258, w_err=6.15946\n",
      "10 0.1\n",
      "./tensorboard/lstm_demo/hidden=10_noise=0.10_27-Apr-15-05-11\n",
      "epoch 0, loss=13.4522, w_err=9.96355\n",
      "epoch 1, loss=11.229, w_err=9.01262\n",
      "epoch 2, loss=8.78038, w_err=8.70215\n",
      "epoch 3, loss=7.7808, w_err=8.37976\n",
      "epoch 4, loss=7.8252, w_err=7.9016\n",
      "epoch 5, loss=8.10587, w_err=7.63238\n",
      "epoch 6, loss=8.18842, w_err=8.39663\n",
      "epoch 7, loss=6.66475, w_err=9.0789\n",
      "10 0.5\n",
      "./tensorboard/lstm_demo/hidden=10_noise=0.50_27-Apr-15-06-32\n",
      "epoch 0, loss=9.28773, w_err=10.0977\n",
      "epoch 1, loss=7.53738, w_err=9.18202\n",
      "epoch 2, loss=7.31514, w_err=9.48958\n",
      "epoch 3, loss=8.12374, w_err=9.79273\n",
      "epoch 4, loss=6.61043, w_err=10.3162\n",
      "epoch 5, loss=6.2345, w_err=10.7097\n",
      "epoch 6, loss=6.7748, w_err=11.1536\n",
      "epoch 7, loss=7.05558, w_err=12.0516\n",
      "100 0\n",
      "./tensorboard/lstm_demo/hidden=100_noise=0.00_27-Apr-15-08-01\n",
      "epoch 0, loss=166.696, w_err=37.0547\n",
      "epoch 1, loss=163.483, w_err=37.5432\n",
      "epoch 2, loss=163.748, w_err=38.6661\n",
      "epoch 3, loss=164.669, w_err=40.3945\n",
      "epoch 4, loss=163.241, w_err=42.4965\n",
      "epoch 5, loss=164.016, w_err=45.5316\n",
      "epoch 6, loss=164.229, w_err=48.6907\n",
      "epoch 7, loss=160.848, w_err=51.9719\n",
      "100 0.1\n",
      "./tensorboard/lstm_demo/hidden=100_noise=0.10_27-Apr-15-09-36\n",
      "epoch 0, loss=152.411, w_err=36.7458\n",
      "epoch 1, loss=151.963, w_err=37.8405\n",
      "epoch 2, loss=150.801, w_err=39.578\n",
      "epoch 3, loss=151.796, w_err=42.3554\n",
      "epoch 4, loss=150.944, w_err=44.0022\n",
      "epoch 5, loss=150.823, w_err=50.8468\n",
      "epoch 6, loss=149.591, w_err=56.6637\n",
      "epoch 7, loss=153.907, w_err=63.4648\n",
      "100 0.5\n",
      "./tensorboard/lstm_demo/hidden=100_noise=0.50_27-Apr-15-11-22\n",
      "epoch 0, loss=156.315, w_err=39.0937\n",
      "epoch 1, loss=153.692, w_err=39.9888\n",
      "epoch 2, loss=150.936, w_err=40.3695\n",
      "epoch 3, loss=151.607, w_err=41.4328\n",
      "epoch 4, loss=153.214, w_err=42.1709\n",
      "epoch 5, loss=156.28, w_err=47.4637\n",
      "epoch 6, loss=151.516, w_err=50.8112\n",
      "epoch 7, loss=152.815, w_err=53.087\n",
      "1000 0\n",
      "./tensorboard/lstm_demo/hidden=1000_noise=0.00_27-Apr-15-16-58\n",
      "epoch 0, loss=4452.39, w_err=137.673\n",
      "epoch 1, loss=4259.04, w_err=139.61\n",
      "epoch 2, loss=4167.66, w_err=139.733\n",
      "epoch 3, loss=4120.18, w_err=139.74\n",
      "epoch 4, loss=4095.25, w_err=139.739\n",
      "epoch 5, loss=4079.51, w_err=139.737\n",
      "epoch 6, loss=4049.03, w_err=139.736\n",
      "epoch 7, loss=4060.32, w_err=139.735\n",
      "1000 0.1\n",
      "./tensorboard/lstm_demo/hidden=1000_noise=0.10_27-Apr-15-27-34\n",
      "epoch 0, loss=4638.88, w_err=138.424\n",
      "epoch 1, loss=4434.76, w_err=140.245\n",
      "epoch 2, loss=4356.57, w_err=140.36\n",
      "epoch 3, loss=4319.93, w_err=140.366\n",
      "epoch 4, loss=4305.19, w_err=140.367\n",
      "epoch 5, loss=4284.77, w_err=140.366\n",
      "epoch 6, loss=4270.08, w_err=140.365\n",
      "epoch 7, loss=4262.39, w_err=140.364\n",
      "1000 0.5\n",
      "./tensorboard/lstm_demo/hidden=1000_noise=0.50_27-Apr-15-38-26\n",
      "epoch 0, loss=3162.13, w_err=139.425\n",
      "epoch 1, loss=2890.35, w_err=141.342\n",
      "epoch 2, loss=2830.92, w_err=141.485\n",
      "epoch 3, loss=2813.56, w_err=141.501\n",
      "epoch 4, loss=2761.54, w_err=141.508\n",
      "epoch 5, loss=2744.51, w_err=141.507\n",
      "epoch 6, loss=2735.56, w_err=141.509\n",
      "epoch 7, loss=2735.56, w_err=141.509\n",
      "lol\n"
     ]
    }
   ],
   "source": [
    "for n_hidden_dim in [10, 100, 1000]:\n",
    "    for noise_level in [0, 0.1, 0.5]:\n",
    "        print(n_hidden_dim, noise_level)\n",
    "        train_lstm_with_noise(noise_level, n_hidden_dim)\n",
    "print(\"lol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3: Neural Tolstoy Model (a character-prediction LSTM)\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this part you will train a character prediction LSTM that predicts the next character given previous characters. \n",
    "The training data is from the book \"War and Peace\" by Leo Tolstoy. \n",
    "\n",
    "### Q3.\n",
    "Compared to the LSTM you implemented in the previous part, the main difference in the character prediction LSTM is that it predicts *discrete* outputs (characters, or their indices in the vocabulary), therefore it is a classification model. As you may recall, the usual choice of loss function for classification is softmax + cross entropy.\n",
    "\n",
    "We have already provided functions for loading the training data. Please define your discrete LSTM in `build_lstm_discrete_prediction_model()`.\n",
    "\n",
    "Hints: \n",
    "- Your LSTM should predict a single character at a time. (That's why it's called a character prediction LSTM.) We are not considering  many-to-many LSTMs here.\n",
    "- Feeding the previous input into the LSTM should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-a37e44213ba2>, line 94)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-a37e44213ba2>\"\u001b[0;36m, line \u001b[0;32m94\u001b[0m\n\u001b[0;31m    c_bar_t = tf.tanh(tf.matmul(x_t, w_c) + tf.matmul(h_prev, u_c)  #(batch, hidden)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tolstoy_reader\n",
    "\n",
    "def get_default_gpu_session(fraction=0.333):\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = fraction\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "def run_tolstoy_train(n_hid):\n",
    "    # generate data\n",
    "    btg, map_dict, backmap_dict = \\\n",
    "        tolstoy_reader.batch_tolstoy_generator(batch_size=200, seq_size=100)\n",
    "    shape = dict(n_steps_per_batch=100, n_unique_ids=len(map_dict), n_hidden_dim=n_hid)\n",
    "    # define LSTM\n",
    "    model = build_lstm_discrete_prediction_model(shape)\n",
    "    \n",
    "    #logdir = './tensorboard/tolstoy'  # if on Windows\n",
    "    logdir = './tensorboard/tolstoy'  # if on Unix\n",
    "    try:\n",
    "        os.makedirs(logdir)\n",
    "    except os.error:\n",
    "        pass\n",
    "    time_now = datetime.datetime.now().strftime(\"%d-%b-%H-%M-%S\")\n",
    "    run_name = 'hidden=%d' % n_hid\n",
    "    sum_path = os.path.join(logdir, run_name + '_' + time_now)\n",
    "    print(sum_path)\n",
    "    max_iter_i = 0\n",
    "    with model['graph'].as_default() as g, get_default_gpu_session(0.9) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sum_writer = tf.summary.FileWriter(sum_path, g)\n",
    "        for epoch_i in range(10):\n",
    "            for iter_i, data_batch in enumerate(btg):\n",
    "                max_iter_i = max(iter_i, max_iter_i)\n",
    "                global_step = epoch_i*max_iter_i+iter_i\n",
    "                \n",
    "                # run training step\n",
    "                train_feed_dict = dict(zip(model['inputs'], data_batch))\n",
    "                to_compute = [model['train_op'], model['summ'], model['loss']]\n",
    "                _, summ, loss_val = sess.run(to_compute, train_feed_dict)\n",
    "                \n",
    "                # for tensorboard\n",
    "                sum_writer.add_summary(summ, global_step)\n",
    "                sum_writer.flush()\n",
    "                \n",
    "                # display loss\n",
    "                if iter_i % 100 == 0:\n",
    "                    print(loss_val, end=', ')\n",
    "                if iter_i % 1000:\n",
    "                    continue\n",
    "                # test generation\n",
    "                pred_length = 50\n",
    "                data_input = next(iter(btg))[0][[0]]\n",
    "                original_sample = data_input.copy()\n",
    "                pred_seq = []\n",
    "                for _ in range(pred_length):\n",
    "                    pred = sess.run(model['pred'], {model['inputs'][0]: data_input})\n",
    "                    pred_seq.append(pred[0])\n",
    "                    data_input = np.roll(data_input, -1, axis=1)\n",
    "                    data_input[0, -1] = pred[0]\n",
    "                print('[%d] Input text:' % (iter_i))\n",
    "                print(''.join([backmap_dict[x] for x in original_sample[0]]))\n",
    "                print('[%d] Generated continuation:' % (iter_i))\n",
    "                print(''.join([backmap_dict[x] for x in pred_seq]))\n",
    "                print(pred_seq)\n",
    "                print()\n",
    "                \n",
    "def build_lstm_discrete_prediction_model(shape):\n",
    "    # shape is dict with keys:\n",
    "    # n_steps_per_batch, n_unique_ids, n_hidden_dim\n",
    "    with tf.Graph().as_default() as g:\n",
    "        X = tf.placeholder(tf.int64, [None, shape['n_steps_per_batch']])\n",
    "        y = tf.placeholder(tf.int64, [None])\n",
    "        \n",
    "        ################################################################\n",
    "        ####################   PUT YOUR CODE HERE   ####################\n",
    "        # define LSTM parameters (scope: weights)\n",
    "        with tf.variable_scope('weights'):\n",
    "            h_0 = tf.get_variable('h_9', [1, shape['n_hidden_dim']])\n",
    "            c_0 = tf.get_variable('c_0', [1, shape['n_hidden_dim']])\n",
    "            w_i = tf.get_variable('w_i', [1, shape['n_hidden_dim']])\n",
    "            w_c = tf.get_variable('w_c', [1, shape['n_hidden_dim']])\n",
    "            w_f = tf.get_variable('w_f', [1, shape['n_hidden_dim']])\n",
    "            w_o = tf.get_variable('w_o', [1, shape['n_hidden_dim']])\n",
    "            u_i = tf.get_variable('u_i', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_c = tf.get_variable('u_c', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_f = tf.get_variable('u_f', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            u_o = tf.get_variable('u_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            v_o = tf.get_variable('v_o', [shape['n_hidden_dim'], shape['n_hidden_dim']])\n",
    "            \n",
    "        h_prev = h_0\n",
    "        c_prev = c_0\n",
    "        for t in range(shape['n_steps_per_batch']):\n",
    "            x_t = tf.cast(X[:, t], tf.float32)\n",
    "            i_t = tf.sigmoid(tf.matmul(x_t,w_i) + tf.matmul(h_prev,u_i))  #(batch, hidden)\n",
    "            c_bar_t = tf.tanh(tf.matmul(x_t, w_c) + tf.matmul(h_prev, u_c))  #(batch, hidden)\n",
    "            f_t = tf.sigmoid(tf.matmul(x_t, w_f) + tf.matmul(h_prev, u_f))  #(batch, hidden)\n",
    "            c_t = i_t * c_bar_t + c_prev * f_t  #(batch, hidden)\n",
    "            o_t = tf.sigmoid(tf.matmul(x_t,w_o) + tf.matmul(h_prev,u_o) + tf.matmul(c_t,v_o))  #(batch, hidden)\n",
    "            h_t = o_t * tf.tanh(c_t) #(batch, hidden)\n",
    "            h_prev = h_t  #(batch, hidden)\n",
    "            c_prev = c_t  #(batch, hidden)\n",
    "            \n",
    "        y_logits = h_t  # compute logits for each discrete output\n",
    "        logits = tf.nn.softmax(y_logits),\n",
    "        pred = tf.argmax(logits, axis=1)\n",
    "        y_dict = dict(labels=y, logits=y_logits)\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict))  # compute loss with respect to (logits, y)\n",
    "        summ = tf.summary.scalar('loss_summ', loss)\n",
    "        train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "        ################################################################\n",
    "\n",
    "        # pred, train_op\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    return {'inputs': [X, y], 'loss': loss, 'train_op': train_op, 'summ': summ,\n",
    "            'graph': g, 'pred': pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the LSTM and see what it says! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tensorboard/tolstoy/hidden=200_27-Apr-17-39-44\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iter() returned non-iterator of type 'GeneratorRestartHandler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-feecdd661e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_tolstoy_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-7e21c5d536a0>\u001b[0m in \u001b[0;36mrun_tolstoy_train\u001b[0;34m(n_hid)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msum_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0miter_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbtg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mmax_iter_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_i\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax_iter_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0miter_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iter() returned non-iterator of type 'GeneratorRestartHandler'"
     ]
    }
   ],
   "source": [
    "hidden = 200\n",
    "run_tolstoy_train(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: Multi-modal Restricted Boltzmann Machines\n",
    "\n",
    "(30 points)\n",
    "\n",
    "- Relevant reading: Goodfellow chapter 20 through 20.8, in particular 20.2 and 20.3. Also see the [lecture notes](https://drive.google.com/file/d/0B8xkaMshuaF9dFVRclo3RENKdUk/view?usp=sharing).\n",
    "\n",
    "For this question, we will consider designing an unsupervised probability distribution  over two modalities.  Suppose we have data consisting of images and their associated captions, so that each input data point may be thought of as the pair $(t,v)$, where $t = (t_1, ..., t_n)$ is the text data and $v = (v_1, ..., v_m)$ is the visual data.  For simplicity, we will consider both as vector inputs (so, for instance, we will not be considering any two-dimensional convolutions for the visual data); the main difference between the two inputs is that the text input is discrete-valued, where we assume that each $t_i$ can take on $k$ different values, and each visual input is real-valued.\n",
    "\n",
    "Following our discussion of Restricted Boltzmann Machines (RBMs), we will further consider a hidden layer with **binary** hidden units $(h_1, ..., h_d)$, and define a joint probability distribution over an input data point and a hidden layer.  In particular, let us define two matrices $W_t\\in\\mathbb{R}^{n\\times d}$ and $W_v\\in\\mathbb{R}^{m\\times d}$; these matrices correspond to weights between text input and hidden units, and weights between visual input and hidden units, respectively.  In addition, we have the bias vectors $a$, $b$, and $c$, associated with the text, visual, and hidden units, respectively.\n",
    "The multi-modal RBM is illustrated below.\n",
    "\n",
    "<img src=\"MultimodalRBM.png\" width=\"550\">\n",
    "\n",
    "Next we define the joint probability distribution $p(t,v,h)$ over text inputs, visual inputs, and hidden states.  First we let the energy function be: \n",
    "\n",
    "$E(v,t,h) = -t^T W_t h - v^T W_v h - a^T t + (0.5 b^Tb - b^T v + 0.5 v^T v) - c^T h$.\n",
    "\n",
    "Then let the joint probability be defined as:\n",
    "\n",
    "$p(v,t,h) = \\frac{1}{Z}exp(-E(v,t,h)),$\n",
    "\n",
    "where $Z$ is the normalizing constant that ensures that the probability distribution is properly normalized to sum/integrate to one.\n",
    "\n",
    "Our goal is to perform maximum likelihood estimation for the parameters $W_t$, $W_v$, and $c$ (we will assume $a$ and $b$ are fixed and known), given a set of $N$ data points $D = \\{(t^{(1)},v^{(1)}), ..., (t^{(N)},v^{(N)})\\}$.  As is typical in such settings, we assume that the data in $D$ are i.i.d. samples.\n",
    "\n",
    "Please write your solutions in the cells below, or hand in a hard copy.\n",
    "For each question you must show your work to receive full credit.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.1 \n",
    "\n",
    "Write down the marginal probability for a single data point $(t^{(1)}, v^{(1)})$, namely $p(t^{(1)}, v^{(1)})$. Treating this probability as the likelihood for a single point, write down the log-likelihood for the entire data set $D$ as a function of the parameters $(W_t, W_v,  c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Solution:**<br>\n",
    "$p(v,t,h)\\ =\\ \\frac{1}{Z}exp(-E(v,t,h))$<br>\n",
    "$p(v,t)\\ =\\ \\frac{\\sum_hexp(-E(v,t,h))}{Z}$<br>\n",
    "\n",
    "\n",
    "$p(v,t,h)\\ =\\ p(h\\ |\\ v,t)\\ p(v,t)$<br>\n",
    "$p(h\\ |\\ v,t)\\ =\\ \\frac{p(v,t,h)}{p(v,t)}\\ =\\ \\frac{exp(-E(v,t,h))}{Z\\ p(v,t)}\\ =\\ \\frac{exp[t^T W_t h + v^T W_v h + a^T t - (0.5 b^Tb - b^T v + 0.5 v^T v) + c^T h]}{Z\\ p(v,t)}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac{exp[t^T W_t h + v^T W_v h + c^T h]}{Z'}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac{exp[\\sum{{(t^T W_t)}_i h_i}+\\sum{{(v^T W_v)}_ih_i}+\\sum c_i h_i]}{Z'}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac 1{Z'}\\prod{exp({(t^T W_t)}_i h_i+{{(v^T W_v)}_ih_i} + c_i h_i)}$<br>\n",
    "$$\n",
    "\\begin{cases}\n",
    "p(h_i=1\\ |\\ v,t)\\ &=&\\ \\frac {\\tilde p(h_i=1\\ |\\ v,t)}{{\\tilde p(h_i=1\\ |\\ v,t)} + {\\tilde p(h_i=0\\ |\\ v,t)}} \\\\\n",
    "&=& \\frac{exp({(t^T W_t)}_i +{{(v^T W_v)}_i} + c_i)}{exp(0) + {exp({(t^T W_t)}_i+{{(v^T W_v)}_i} + c_i)}}  \\\\\n",
    "&=& \\sigma({(t^T W_t)}_i +{{(v^T W_v)}_i} + c_i)  \\\\\n",
    "p(h_i=0\\ |\\ v,t)\\ &=& \\sigma(-{(t^T W_t)}_i - {{(v^T W_v)}_i} - c_i)  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$p(h\\ |\\ v, t)\\ =\\prod_{i=1}^{n_h}{\\sigma{\\left((2h-1)\\dot({t^T W_t} - {v^T W_v} - c)\\right)}_i}$<br>\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\begin{array}{rcl}\n",
    "p(v,t)\\ &=& \\frac{p(h\\ |\\ v,t)}{p(v,t,h)}\\\\\n",
    "&=&\\frac{\\prod_{i=1}^{n_h}{\\sigma{\\left((2h-1)\\dot({t^T W_t} - {v^T W_v} - c)\\right)}_i}}{\\frac{1}{Z}exp(-E(v,t,h))}\\ or\\ \\frac{\\sum_hexp(-E(v,t,h))}{Z}\\\\\n",
    "\\log p(D)\\ &=& \\sum_{j=0}^{j=n_D}\\log p(v^{(i)},t^{(i)})\\\\\n",
    "&=&\\sum_{j=0}^{j=n_D}\\left(\\log{\\prod_{i=1}^{n_h}{\\sigma{\\left((2h-1)\\dot({t^T W_t} - {v^T W_v} - c)\\right)}_i}}-\\log{\\frac{1}{Z}exp(-E(v,t,h))}\\right)^{(j)} \\\\\n",
    "&=&\\sum_{j=0}^{j=n_D}\\left(\\log{\\prod_{i=1}^{n_h}{\\sigma{\\left((2h-1)\\dot({t^T W_t} - {v^T W_v} - c)\\right)}_i}}\\right)^{(i)} \\\\\n",
    "&=&\\sum_{j=0}^{j=n_D}{\\sum_{i=1}^{n_h}{\\log\\Big(\\sigma{\\left((2h-1)\\dot({t^T W_t} - {v^T W_v} - c)\\right)_i\\Big)}^{(j)}}} \\\\\n",
    "\\end{array}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.2 \n",
    "\n",
    "Write down the partial derivative of $E(t,v,h)$ with respect to each of the parameters, i.e., each entry of $W_t$, each entry of $W_v$, and each entry of $c$. You can also directly write in matrix/vector forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "$E(v,t,h) = -t^T W_t h - v^T W_v h - a^T t + (0.5 b^Tb - b^T v + 0.5 v^T v) - c^T h$<br>\n",
    "$\\left(\\frac{\\partial E(v,t,h)}{\\partial W_t}\\right)_{ij}\\ =\\ t_ih_i$<br>\n",
    "$\\left(\\frac{\\partial E(v,t,h)}{\\partial W_v}\\right)_{ij}\\ =\\ v_ih_i$<br>\n",
    "$\\left(\\frac{\\partial E(v,t,h)}{\\partial c}\\right)_{i}\\ =\\ h_i$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.3 \n",
    "\n",
    "Derive the conditional distributions $p(h | v, t)$, $p(v | h)$ and $p(t | h)$, either in elementwise form or in vector form.\n",
    "Also establish that $v$ and $t$ are conditionally independent given $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Solution:**<br>\n",
    "From Q4.1 we already have:<br>\n",
    "$p(h\\ |\\ v, t)\\ =\\prod_{i=1}^{n_h}{\\sigma{\\left((2h-1)\\dot({(t^T W_t)}_i - {(v^T W_v)}_i - c_i)\\right)}_i}$<br>\n",
    "\n",
    "Derive $p(v,t\\ |\\ h)$ in the same way, then we have:<br>\n",
    "$p(v,t\\ |\\ h)\\ =\\ \\frac{p(v,t,h)}{p(h)}\\ =\\ \\frac{exp(-E(v,t,h))}{Z\\ p(h)}\\ =\\ \\frac{exp[t^T W_t h + v^T W_v h + a^T t - (0.5 b^Tb - b^T v + 0.5 v^T v) + c^T h]}{Z\\ p(h)}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac{exp[t^T W_t h + v^T W_v h + a^T t + b^T v - 0.5 v^T v)]}{Z'}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac{exp[\\sum{t_i (W_th)_i} + \\sum{v_i (W_vh)_i}+\\sum \\sum{a_i t_i}+\\sum{b_iv_i}-\\sum 0.5 v_i^2]}{Z'}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac 1{Z'}\\prod_{i=1}^{n_{v,t}}{exp(t_i (W_th)_i + v_i (W_vh)_i+a_i t_i+b_iv_i - 0.5 v_i^2)}$<br>\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "p(v_i=1, t_i=1\\ |\\ h)\\ &=&\\ \\frac {\\tilde p(v_i=1,t_i=1\\ |\\ h)}{{\\tilde p(v_i=1, t_i=1\\ |\\ h)} + {\\tilde p(v_i=0, t_i=0\\ |\\ h)} + {\\tilde p(v_i=0, t_i=1\\ |\\ h)} + {\\tilde p(v_i=1, t_i=0\\ |\\ h)}} \\\\\n",
    "&=& \\frac{exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5)}{exp(a_i+(W_th)_i) + exp(0) + exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5) + exp((W_vh)_i+b_i- 0.5)}\\\\\n",
    "p(v_i=0, t_i=0\\ |\\ h)\\ &=& \\frac{exp(0)}{exp(a_i+(W_th)_i) + exp(0) + exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5) + exp((W_vh)_i+b_i- 0.5)}\\\\\n",
    "p(v_i=0, t_i=1\\ |\\ h)\\ &=& \\frac{exp(a_i+(W_th)_i)}{exp(a_i+(W_th)_i) + exp(0) + exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5) + exp((W_vh)_i+b_i- 0.5)}\\\\\n",
    "p(v_i=1, t_i=0\\ |\\ h)\\ &=& \\frac{exp(b_i-0.5+(W_vh)_i)}{exp(a_i+(W_th)_i) + exp(0) + exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5) + exp((W_vh)_i+b_i- 0.5)}\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "p(v_i=1\\ |\\ h)\\ &=&\\ \\frac {{\\tilde p(v_i=1,t_i=1\\ |\\ h)}+{\\tilde p(v_i=1,t_i=0\\ |\\ h)}}{{\\tilde p(v_i=1,t_i=1\\ |\\ h)}+{\\tilde p(v_i=1,t_i=0\\ |\\ h)} + {\\tilde p(v_i=0,t_i=1\\ |\\ h)}+{\\tilde p(v_i=0,t_i=0\\ |\\ h)}} \\\\\n",
    "&=& \\frac{exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5) + exp((W_vh)_i+b_i- 0.5)}{exp(a_i+(W_th)_i) + exp(0) + exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5) + exp((W_vh)_i+b_i- 0.5)}\\ =\\ \\frac{exp((W_vh)_i+b_i- 0.5)}{1 + {exp((W_vh)_i+b_i- 0.5)}} \\\\\n",
    "&=& \\sigma((W_vh)_i+b_i - 0.5)  \\\\\n",
    "p(v_i=0\\ |\\ h)\\ &=& \\sigma(-b_i + 0.5-(W_vh)_i)  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "p(t_i=1\\ |\\ h)\\ &=&\\ \\frac {{\\tilde p(t_i=1,v_i=1\\ |\\ h)}+{\\tilde p(t_i=1,v_i=0\\ |\\ h)}}{{\\tilde p(v_i=1,t_i=1\\ |\\ h)}+{\\tilde p(v_i=1,t_i=0\\ |\\ h)} + {\\tilde p(v_i=0,t_i=1\\ |\\ h)}+{\\tilde p(v_i=0,t_i=0\\ |\\ h)}} \\\\\n",
    "&=& \\frac{exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5)+exp(a_i+(W_th)_i)}{exp(a_i+(W_th)_i) + exp(0) + exp((W_th)_i+(W_vh)_i+a_i +b_i- 0.5) + exp((W_vh)_i+b_i- 0.5)}\\ =\\ \\frac{exp(a_i+(W_th)_i))}{1 + {exp(a_i+(W_th)_i))}} \\\\\n",
    "&=& \\sigma(a_i+(W_th)_i)  \\\\\n",
    "p(t_i=0\\ |\\ h)\\ &=& \\sigma(-a_i-(W_th)_i))  \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$p(v\\ |\\ h)\\ =\\int\\prod_{i=1}^{n_v}{\\sigma{\\left((2v-1)\\dot({b - 0.5+W_vh})\\right)}_i}dt$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\prod_{i=1}^{n_v}{\\sigma{\\left((2v-1)\\dot({b - 0.5+W_vh})\\right)}_i}$<br>\n",
    "\n",
    "$p(t\\ |\\ h)\\ =\\int\\prod_{i=1}^{n_v}{\\sigma{\\left((2t-1)\\dot{(a+W_th)}\\right)}_i}dv$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\prod_{i=1}^{n_v}{\\sigma{\\left((2t-1)\\dot{(a+W_th)}\\right)}_i}$<br>\n",
    "And $p(v,t\\ |\\ h)\\ =\\ p(v\\ |\\ h)\\dot p(t\\ |\\ h)$. So they are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.4\n",
    "\n",
    "Write down the partial derivative of the log-likelihood for $D$ with respect to each of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "$\\log p(D) = \\sum_{j=0}^{j=n_D}{\\sum_{i=1}^{n_h}{\\log\\Big(\\sigma{\\left((2h-1)\\dot(t^T W_t - v^T W_v - c)\\right)_i\\Big)}^{(j)}}}$<br>\n",
    "$\\frac{\\partial{\\log p(D)}}{\\partial{W_t}}\\ =\\ \\frac{\\partial\\sum_{j=0}^{j=n_D}\\sum_{i=1}^{n_h}\\Big(t^T-t^T\\sigma \\left((2h-1)\\dot(t^T W_t - v^T W_v - c)\\right)_i\\Big)^{(i)}}{\\partial{W_t}}\\ =\\ \\sum_{j=0}^{j=n_D}\\sum_{i=1}^{n_h}\\Big(t^T\\sigma \\left((1-2h)\\dot(t^T W_t - v^T W_v - c)\\right)_i\\Big)^{(j)}$<br>\n",
    "$\\frac{\\partial{\\log p(D)}}{\\partial{W_v}}\\ =\\ \\frac{\\partial\\sum_{j=0}^{j=n_D}\\sum_{i=1}^{n_h}\\Big(v^T-v^T\\sigma \\left((2h-1)\\dot(v^T W_t - v^T W_v - c)\\right)_i\\Big)^{(i)}}{\\partial{W_v}}\\ =\\ \\sum_{j=0}^{j=n_D}\\sum_{i=1}^{n_h}\\Big(t^T\\sigma \\left((1-2h)\\dot(v^T W_t - v^T W_v - c)\\right)_i\\Big)^{(j)}$<br>\n",
    "$\\frac{\\partial{\\log p(D)}}{\\partial{c}}\\ =\\ \\frac{\\partial\\sum_{j=0}^{j=n_D}\\sum_{i=1}^{n_h}\\Big(-1+\\sigma \\left((2h-1)\\dot(v^T W_t - v^T W_v - c)\\right)_i\\Big)^{(i)}}{\\partial{c}}\\ =\\ \\sum_{j=0}^{j=n_D}\\sum_{i=1}^{n_h}\\Big(-\\sigma \\left((1-2h)\\dot(v^T W_t - v^T W_v - c)\\right)_i\\Big)^{(j)}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q4.5 \n",
    "\n",
    "Extend the contrastive divergence approach for standard RBMs, as discussed in class, to suggest a gradient ascent procedure for learning the parameters of our multi-modal RBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Solution:**<br>\n",
    "Extend to Gaussian-Bernoulli RBM, $v\\in\\mathcal{R}^D,\\ t\\in{1,2,..., K}$<br>\n",
    "$p(h\\ |\\ v, t)\\ =\\prod_{i=1}^{n_h}{\\sigma{\\left((2h-1)\\dot({(t^T W_t)}_i - {(v^T W_v)}_i - c_i)\\right)}_i}$<br>\n",
    "<br><br>$p(v,t\\ |\\ h)\\ =\\ \\frac{exp[t^T W_t h + v^T W_v h + a^T t - (0.5 b^Tb - b^T v + 0.5 v^T v) + c^T h]}{Z\\ p(h)}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac 1{Z'}\\prod_{i=1}^{n_{v,t}}{exp(t_i (W_th)_i + v_i (W_vh)_i+a_i t_i+b_iv_i - 0.5 v_i^2)}$<br>\n",
    "<br><br>$p(v|h)\\ =\\ \\sum_t\\frac{exp[t^T W_t h + v^T W_v h + a^T t - (0.5 b^Tb - b^T v + 0.5 v^T v) + c^T h]}{Z\\ p(h)}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\prod_{i=1}^{n_{v,t}}exp(v_i (W_vh)_i+b_iv_i - 0.5 v_i^2)\\sum_t\\frac 1{Z'}\\prod_{i=1}^{n_{v,t}}{exp(t_i (W_th)_i+a_i t_i)}$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac 1{Z''(h)}\\prod_{i=1}^{n_{v,t}}{exp(v_i (W_vh)_i+b_iv_i - 0.5 v_i^2)}$<br>\n",
    "$v|h\\sim\\ \\mathcal{N}(v|h; W_vh+b, exp(-0.5*(W_vh+2b)^T(W_vh+2b))$\n",
    "<br><br>$p(t|h)\\ =\\ \\int_v\\frac{exp[t^T W_t h + v^T W_v h + a^T t - (0.5 b^Tb - b^T v + 0.5 v^T v) + c^T h]}{Z\\ p(h)}dv$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\prod_{i=1}^{n_{v,t}}exp(t_i (W_th)_i+a_i t_i)\\int_v\\frac 1{Z'}\\prod_{i=1}^{n_{v,t}}{exp(v_i (W_vh)_i+b_iv_i - 0.5 v_i^2)}dv$<br>\n",
    "$\\qquad\\ \\ \\ \\ \\ =\\ \\frac 1{Z'''(h)}\\prod_{i=1}^{n_{v,t}}{exp(t_i (W_th)_i+a_i t_i)}$<br>\n",
    "$p(t_i=1|h)\\ =\\ \\frac {exp(t_i (W_th)_i+a_i t_i)}{\\sum_{j=0}^{n_{v,t}}exp(t_j (W_th)_j+a_j t_j)}$<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
