{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">by Weichao Zhou <br> with help of Brian Kulis<br> </h4>\n",
    "<h1>Problem Set 5</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Markov Decision Process (MDP)\n",
    "\n",
    "Markov decision process (MDP) is a discrete time stochastic control process which has been used to modele decision making problems solved via dynamic programming and reinforcement learning. We will review the formal definition of MDP and build a simple MDP model.\n",
    "\n",
    "A Markov Decision Process (MDP) is a tuple $M = (S, A, T, \\gamma, R)$.\n",
    "* $S$ is a finite set of states; \n",
    "* $A$ is a set of actions; \n",
    "* $T : S \\times A \\times S\\rightarrow [0, 1]$ is a transition function describing the probability of transitioning from one state $s\\in S$ to another state by taking action $a\\in A$ in state $s$; \n",
    "* $\\gamma\\in [0, 1)$ is a discount factor indicating how the importance of future reward attenuates.\n",
    "* $R:S\\times A\\rightarrow \\mathbb{R}$ or $R:S\\rightarrow \\mathbb{R}$ is a reward function which maps each state-action pair to a real number indicating the reward of being in state $s$. For simplicity, let's only consider the second case, equivalently assuming $\\forall a, R(s,a)=R(s)$; \n",
    "\n",
    "A deterministic and stationary (or memoryless) policy $\\pi : S \\rightarrow A$ for an\n",
    "MDP $M$ is a mapping from states to actions, i.e. the policy deterministically\n",
    "selects which action to take solely based on the current state. \n",
    "\n",
    "A trajectory $\\tau= s_0 \\rightarrow s_1 \\rightarrow s_2 , \\ldots$, is a sequence of states where each $s_t \\in S$. The accumulated reward of $\\tau$ is $\\sum^\\infty_{t=0} \\gamma^t R(s_t)$.\n",
    "\n",
    "Value function $V_\\pi : S \\rightarrow \\mathbb{R}$ measures the expectation of accumulated reward starting from $s_0$ and following policy $\\pi$. $$V_\\pi(s_0)= \\mathbb{E}_\\pi[\\sum^\\infty_{t=0} \\gamma^t R(s_t)]$$. \n",
    "\n",
    "Q-function $Q_\\pi: S\\times A\\rightarrow \\mathbb{R}$ measures the expectation of accumulated reward of performing action $a_0\\in A$ in state $s_0\\in S$ at first step and then following policy $\\pi$ afterwards. $$Q_\\pi(s_0, a_0)= R(s_0) +  \\gamma\\cdot\\sum_{s_1\\in S}T(s_0, a_0, s_1)\\mathbb{E}_\\pi[\\sum^\\infty_{t=1} \\gamma^{t-1} R(s_t)]$$ \n",
    "\n",
    "According to **``Bellman Equations``**, for all $s\\in S, a\\in A$, $V_\\pi$ and $Q_\\pi$ satisfies:\n",
    "* $V_\\pi(s)=R(s)+\\gamma\\sum_{s'}T(s, \\pi(s), s')V_\\pi(s')$\n",
    "* $Q_\\pi(s,a)=R(s)+\\gamma\\sum_{s'}T(s, a, s') V_\\pi(s')$\n",
    "\n",
    "\n",
    "An optimal policy $\\pi$ for MDP $M$ is a policy that maximizes the Q-function at every state: \n",
    "* $\\pi(s)\\in argmax_{a\\in A} Q_\\pi(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1**: For an MDP with finite state space and action space, we can use matrices to represent the mapping relations for transition function, reward function, policy, value function and Q-function. Please find the `mdp.py` file and finish the function below that implements the second Bellman equation as shown above. This function should take a value function(matrix) as input, solve the corresponding Q-function(matrix), then output a myopic policy $argmax_{a\\in A} Q(s,a)$ as well as its corresponding state-action value $max_{a\\in A} Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "from mdp import MDP\n",
    "\n",
    "    \n",
    "def BellmanUpdate(self, V=None):\n",
    "    # Apply the Bellman operation on the value function V to make a single\n",
    "    # update on the value function and policy.\n",
    "    #\n",
    "    # Returns: (policy, value), tuple of improved policy and its value\n",
    "    #\n",
    "    if V is None:\n",
    "        # this V should be a reference to the data rather than a copy\n",
    "        V = self.V\n",
    "    \n",
    "    try:\n",
    "        assert V.shape in ((self.S,), (1, self.S)), (\n",
    "        \"Warning: shape of V is not right shape.\")\n",
    "    except AttributeError:\n",
    "        raise TypeError(\"V must be a numpy array or matrix.\")\n",
    "    ## Initialize an empty Q-function\n",
    "    Q = np.empty((self.A, self.S))\n",
    "\n",
    "    ## Implement the second Bellman Equation: use reward function and value function \n",
    "    ## to represent the Q-function.\n",
    "\n",
    "    ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "    raise Exception(\"At most two line of code is needed here. \\\n",
    "    Refer to the secton Bellman Equation mentioned above. \\\n",
    "    Solve the Q-function by using reward function self.M.R, \\\n",
    "    transition function self.M.T \\\n",
    "    and value function V.\")\n",
    "    ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "    \n",
    "    return (Q.argmax(axis=0), Q.max(axis=0))\n",
    "\n",
    "MDP.BellmanUpdate = BellmanUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2:** Now we have a $8\\times 8$ Grid World based on the MDP we built in Q1.1. An agent can start from the upper-left corner and moves from cell to cell until it reaches the lower-right corner. At each step, the agent can choose to stay in current cell or move to an adjacent cell but with 20% chance of moving randomly. Please run the following code to draw a reward mapping of the grid world in grey scale and draw a reward-state plot with its y-axle being the reward and the x-axle being the state index. Think about how to solve a policy that can maximally gain the accumulated rewards. Note that light cells have higher rewards than dark cells. The program for the grid world can be found in `mdp.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mdp import gridworld \n",
    "\n",
    "\n",
    "###Let's draw the gridworld\n",
    "grids = gridworld(8)\n",
    "\n",
    "###Let's draw the reward mapping of the gridworld\n",
    "grids.draw_grids()\n",
    "\n",
    "###Let's draw a plot of the rewards. \n",
    "##The x-axle is the index of states.\n",
    "##The y-axle is the reward.\n",
    "grids.draw_plot(rewards = grids.M.R)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set, we will implement policy iteration algorithm. Recall the policy iteration algorithm. Please find the `PolicyIteration` class in `mdp.py`. The algorithm is based on the MDP we have previously built. Especially, the `BellmanUpdate` function in **Q1.1** will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1:** Complete the function below. The goal is to calculate the value function of a given policy through iteration. When the policy is given, the transition probability becomes $P(s, s')= T(s, \\pi(s), s')$. Use $P$ and reward function $R$ to solve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import PolicyIteration\n",
    "\n",
    "def ValueUpdate(self, epsilon = 1E-10, max_iter = 10000):\n",
    "        # Calculate the value function of the policy iteratively.\n",
    "        #\n",
    "        # Observe the Bellman Equation.\n",
    "        # The policy, rewards, transition probabilities are all given.\n",
    "        # Value function can be calculated iteratively until convergence.\n",
    "        # Think about why does it converge?????\n",
    "        if epsilon is None:\n",
    "            epsilon = self.M.epsilon\n",
    "        if max_iter is None:\n",
    "            max_iter = self.M.max_iter  \n",
    "            \n",
    "        # The transition probability determined by the policy\n",
    "        P = self.TransitionUpdate()\n",
    "        assert P.shape == (self.M.S, self.M.S)\n",
    "        # Reset the Value function to be equal to the Reward function\n",
    "        self.M.V = self.M.R.copy()\n",
    "        \n",
    "        itr = 0\n",
    "        \n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "        raise Exception(\"Use reward function self.M.R and transition probability P\\ \n",
    "        to iteratively solve the value function self.M.V under the current policy\\ \n",
    "        till converge.\")\n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "        return self.M.V\n",
    "\n",
    "PolicyIteration.ValueUpdate = ValueUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2:** Implement the policy iteration algorithm to solve the optimal policy for the $8\\times 8$ grid world. Please run the following code to draw the value mapping in grey scale, the value-state plot as well as the optimal policy diagram where different actions are indicated by different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Let's initialize a Reinforcement Learning agent that implement Policy Iteration\n",
    "agent = PolicyIteration(grids.M, policy_init = None)\n",
    "agent.iterate()\n",
    "\n",
    "###Let's draw the value mapping of the gridworld\n",
    "grids.draw_grids(grids.M.V)\n",
    "\n",
    "###Let's draw the value plot of the states\n",
    "grids.draw_plot(values = grids.M.V)\n",
    "\n",
    "###Let's draw the optimal policy of the gridworld\n",
    "grids.draw_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3:** Besides using dynamic programming, we can solve value function of a given policy through a linear equation. Observe  Bellman Equation and note that the value function, transition probabilities and reward functions are all matricies. Please use **one line** of code to solve the value function of a given policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueUpdate_LQ(self):\n",
    "    # Calculate the value function of the policy by solving a linear equation.\n",
    "    #\n",
    "    # Observe the Bellman Equation. \n",
    "    # The policy, rewards, transition probabilities are all given. \n",
    "    # Can you solve the value function(matrix) by solving a linear equation?\n",
    "    # Think about how to do it. \n",
    "    \n",
    "    P = self.TransitionUpdate()\n",
    "    \n",
    "    ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "    raise Exception(\"Use one line of code to solve the value function from a linear equation\")\n",
    "    ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "    return self.M.V\n",
    "\n",
    "PolicyIteration.ValueUpdate = ValueUpdate_LQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.4:** Use the code in Q2.3 to replace the function in Q2.1 and Redo Q2.2. The generated graphs should be almost the same as Q2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Let's initialize a Reinforcement Learning agent that implement Policy Iteration\n",
    "grids = None\n",
    "agent = None\n",
    "\n",
    "grids = gridworld(8)\n",
    "agent = PolicyIteration(grids.M, policy_init = None)\n",
    "agent.iterate()\n",
    "\n",
    "###Let's draw the value mapping of the gridworld\n",
    "grids.draw_grids(grids.M.V)\n",
    "\n",
    "###Let's draw the value plot of the states\n",
    "grids.draw_plot(values = grids.M.V)\n",
    "\n",
    "###Let's draw the optimal policy of the gridworld\n",
    "grids.draw_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Deep Q-Learning\n",
    "\n",
    "Policy Iteration may be efficient in solving small scale RL problems where the transition model is given. However, it is expensive to solve large scale RL problem where transition function is not explicitly known. In this problem set, we will use deep Q-learning to solve an optimal policy for a large grid world. Now we wrap up the grid world and treat it as an unknown environment. A learning agent can interact with the environment by performing an actions and receiving from the environment an observation(or state), reward and other information. Please run the program below to observe how the interaction works. Although the size of grid world is set to 50x50, you can change it to any large number as you like. Just be aware that when the size is too large, the environment may takes longer time to give the feed back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From state:[0], perform action:1, to state:[1], get reward:[0.91435099] \n",
      "From state:[1], perform action:3, to state:[0], get reward:[0.93359397] \n",
      "From state:[0], perform action:4, to state:[50], get reward:[0.91435099] \n",
      "From state:[50], perform action:0, to state:[50], get reward:[0.91435099] \n",
      "From state:[50], perform action:1, to state:[51], get reward:[0.89469423] \n",
      "From state:[51], perform action:0, to state:[51], get reward:[0.89469423] \n",
      "From state:[51], perform action:1, to state:[101], get reward:[0.87546043] \n",
      "From state:[101], perform action:3, to state:[101], get reward:[0.87546043] \n",
      "From state:[101], perform action:3, to state:[100], get reward:[0.89553947] \n",
      "From state:[100], perform action:2, to state:[150], get reward:[0.87718718] \n"
     ]
    }
   ],
   "source": [
    "from mdp import wrapper\n",
    "import random\n",
    "\n",
    "## First wrap the gridworld\n",
    "env = wrapper(gridworld(50))\n",
    "num_actions = env.num_actions()\n",
    "\n",
    "# Just run the code and observe the data types and shapes of the variables\n",
    "state = env.reset()\n",
    "for j in range(10):\n",
    "    action = random.randint(0, num_actions - 1)\n",
    "    state_, reward, done, _ = env.step(action)\n",
    "    print(\"From state:{}, perform action:{}, to state:{}, get reward:{} \".format(state, action, state_, reward))\n",
    "    state = state_.copy()\n",
    "    if done:\n",
    "        break \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1:** Now let's build a simple Deep Q-network. Please make sure that torch is installed in your computer. Use the function below to design the network layers. You can use MLP, CNN, or whatever you like. But make sure that you understand <a href=\"https://www.nature.com/articles/nature14236.pdf\">how DQN works</a>, especially what are the input and output. **You are only required to build the neural network layers.** Please refer to the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "\n",
    "## Theoretically, you can build what ever network as you like, \n",
    "## but make sure that the I/O size must be compatible.\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            input_size (int): What is the input of the Q Network?? What is the size of the input???\n",
    "            output_size (int): What is the output of the Q Network?? What is the size of the output???\n",
    "            seed (int): Random seed\n",
    "        Example\n",
    "        =======\n",
    "        self.l1 = nn.Linear(input_size, 16)\n",
    "        self.l2 = nn.Linear(16, output_size)\n",
    "        \"\"\"\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "        raise Exception(\"Design your Neural Network layers\")\n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Build a network that maps input -> output.\n",
    "        Example\n",
    "        =======            \n",
    "        x = func.relu(self.l1(input_data))\n",
    "        x = func.relu(self.l2(x))\n",
    "        return x\n",
    "        \"\"\"\n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "        raise Exception(\"Design your Neural Network layers\")\n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2**: Now let's build a simple DQN agent. The agent will \n",
    "\n",
    "* interact with the environment\n",
    "* use Q-Network to choose actions\n",
    "\n",
    "You are only required to finish a few lines of code. After a batch of experiences are read from the buffer, what should the neural network do next? How to define the loss function? Please understand what the classes and functions are doing. You can change the parameters as you like.\n",
    "\n",
    "In the end, please \n",
    "* **write down the loss function of Deep-Q-Learning Network**\n",
    "* **explain why do we need two Q-networks in this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "batch_size = 10000 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "gamma = .99 #Discount factor on the target Q-values\n",
    "e_start = 1.0 #Starting chance of random action\n",
    "e_end = 0.001 #Final chance of random action\n",
    "annealing_steps = 100. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
    "max_epLength = 300 #The max allowed length of our episode.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "\n",
    "e = e_start\n",
    "e_rate = (e_start - e_end)/annealing_steps\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Reward: 247.40\n",
      "Episode 100\tAverage Reward: 275.29\n",
      "Episode 200\tAverage Reward: 270.81\n",
      "Episode 300\tAverage Reward: 267.38\n",
      "Episode 400\tAverage Reward: 265.33\n",
      "Episode 500\tAverage Reward: 265.24\n",
      "Episode 600\tAverage Reward: 264.62\n",
      "Episode 700\tAverage Reward: 264.51\n",
      "Episode 800\tAverage Reward: 264.05\n",
      "Episode 895\tAverage Reward: 268.81"
     ]
    }
   ],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        # Define memory stack\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # Each experience is a named tuple \n",
    "        self.to_experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Add experience to the memory stack\n",
    "        self.memory.append(self.to_experience(state, action, reward, next_state, done))\n",
    "            \n",
    "    def sample(self, size):\n",
    "        # Randomly choose k experiences\n",
    "        experiences = random.sample(self.memory, k = size)\n",
    "        # Extract the elements of the experiences \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    def __len__(self):\n",
    "        #Return the current size of internal memory.\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "class dqn_agent(object):\n",
    "    def __init__(self, state_size = 1, action_size = 5, seed = 0):\n",
    "        # The size of each state returned by the environment\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # The size of the action to be output to the environment\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.seed = seed\n",
    "        \n",
    "        # Define two Q-networks\n",
    "        self.qnetwork = Q_Network(state_size, action_size, self.seed).to(device)\n",
    "        self.qnetwork_target = Q_Network(state_size, action_size, self.seed).to(device)\n",
    "        \n",
    "        # Define optimizer for one of the Q-networks\n",
    "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=1e-4)\n",
    "\n",
    "        # Define a memory buffer to store experiences\n",
    "        self.myBuffer = experience_buffer()\n",
    "\n",
    "    def choose_action(self, state, epsilon = None):\n",
    "        # Choose one action according to the current state\n",
    "        \n",
    "        if epsilon is None:\n",
    "            epsilon = eps\n",
    "        \n",
    "        # Feed the input\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork.eval()\n",
    "        with torch.no_grad():\n",
    "            Q_value = self.qnetwork(state)\n",
    "        self.qnetwork.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        max_action = np.argmax(Q_value.cpu().data.numpy())\n",
    "        random_action = random.choice(np.arange(self.action_size))\n",
    "        if random.random() > epsilon:\n",
    "            action = max_action\n",
    "        else:\n",
    "            action = random_action\n",
    "            \n",
    "        return action, max_action\n",
    "    \n",
    "    def optimize(self):\n",
    "        # Feed experience to the buffer if it is not full\n",
    "        if len(self.myBuffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        #Get a random batch of experiences.\n",
    "        states, actions, rewards, next_states, dones = self.myBuffer.sample(batch_size) \n",
    "       \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "        raise Exception(\"After a batch of experiences have been read from the buffer, \\\n",
    "                        what should the DQN do next? How to define the loss function?\")\n",
    "        ## >>>>>>>>>>>>>>>>>>>>>code required<<<<<<<<<<<<<<<<<<<<<<<<<<##\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        for target_param, param in zip(self.qnetwork_target.parameters(), self.qnetwork.parameters()):\n",
    "            target_param.data.copy_(tau*param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "        \n",
    "def train(agent): \n",
    "    #create lists to contain total rewards\n",
    "    total_rewards = []\n",
    "    total_steps = 0\n",
    "    e = e_start\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than max_epLength moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            action, _ = agent.choose_action(state, e)\n",
    "            e = max(e_end, e - e_rate)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            #Save the experience to our episode buffer.\n",
    "            agent.myBuffer.add(state, action, reward, next_state, done) \n",
    "\n",
    "            if total_steps % (update_freq) == 0:\n",
    "                agent.optimize()\n",
    "\n",
    "            total_steps += 1\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        #Periodically save the model. \n",
    "        print('\\rEpisode {}\\tAverage Reward: {:.2f}'.format(i, np.mean(total_reward[-10:])), end=\"\")\n",
    "        if i % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Reward: {:.2f}'.format(i, np.mean(total_rewards)))\n",
    "        if np.mean(total_rewards)>=max_epLength:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage reward: {:.2f}'.format(i-100, np.mean(total_rewards)))\n",
    "            break\n",
    "    torch.save(agent.qnetwork.state_dict(), 'checkpoint.pth')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(total_rewards)), total_rewards)\n",
    "    plt.ylabel('Total Rewards')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "                \n",
    "agent = dqn_agent()\n",
    "train(agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3** Check how good the DQN works. Enumerate all the states in the grid world and use the trained DQN to select the action for each state. Then draw the reward mapping graph and policy diagram. It is your freedom to change the parameters and structure of the DQN, or even rewrite the entire program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "\n",
    "agent.qnetwork.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "policy = list()\n",
    "for state in range(env.num_states()):\n",
    "    ##>>>>>>>>>>>>Code Required<<<<<<<<<<<<\n",
    "    raise Exception(\"Which action does the DQN choose in this state?\")\n",
    "    ##>>>>>>>>>>>>Code Required<<<<<<<<<<<<\n",
    "    policy.append(action)\n",
    "    \n",
    "env.render_rewards()\n",
    "env.render_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Actor-Critic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1** Understand what is actor-critic learning. Please review the slides or search materials on line by yourself and\n",
    "* **write down the loss functions** \n",
    "* **explain what do actor and critic respectively mean in this algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.2**. Install Atari Emulator from <a href=\"https://github.com/mgbellemare/Arcade-Learning-Environment\">here</a>. Use a pretrained actor-critic neural network model to play the breakout game. Run the program. There will be a gif image generated in the current folder. Feel free to implement your own deep actor-critic learning agent or use the DQN we have built in Problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "from atari_emulator import AtariEmulator\n",
    "from ale_python_interface import ALEInterface\n",
    "import imagio\n",
    "\n",
    "BIN = \"atari_roms/breakout.bin\"\n",
    "noops = 30\n",
    "test_count = 1\n",
    "\n",
    "writer = imagio.get_writer('breakout.gif', fps = 30)\n",
    "\n",
    "def append_frame(frame):\n",
    "    writer.append_data(frame)\n",
    "\n",
    "def create_environment():\n",
    "    ale_int = ALEInterface()\n",
    "    ale_int.loadROM(str.encode(BIN))\n",
    "    num_actions = len(ale_int.getMinimalActionSet())\n",
    "    return AtariEmulator(BIN), num_actions\n",
    "\n",
    "\n",
    "def choose_next_actions(num_actions, states, session):\n",
    "    policy, value = session.run(['local_learning_2/actor_output_policy:0', 'local_learning_2/critic_output_out:0'], feed_dict = {'local_learning/input:0': states})\n",
    "    policy = policy - np.finfo(np.float32).epsneg\n",
    "\n",
    "    action_indices = [int(np.nonzero(np.random.multinomial(1, p))[0]) for p in policy]\n",
    "\n",
    "    new_actions = np.eye(num_actions)[action_indices]\n",
    "\n",
    "    return new_actions, value, policy\n",
    "\n",
    "\n",
    "def run():\n",
    "    environment, num_actions = create_environment()\n",
    "    environment.on_new_frame = append_frame\n",
    "    \n",
    "    checkpoints_ = \"pretrained/breakout/checkpoints/\"\n",
    "    with tf.Session() as sess:\n",
    "        meta_ = os.path.join(checkpoints_, \"ac.meta\")\n",
    "\n",
    "        saver = tf.train.import_meta_graph(meta_)\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpoints_))\n",
    "\n",
    "        states = np.array([environment.get_initial_state()])\n",
    "        if noops != 0:\n",
    "            for _ in range(random.randint(0, noops)):\n",
    "                state, _, _ = environment.next(environment.get_noop())\n",
    "\n",
    "        episodes_over = np.zeros(test_count, dtype=np.bool)\n",
    "        rewards = np.zeros(1, dtype=np.float32)\n",
    "        while not all(episodes_over):\n",
    "            actions, _, _ = choose_next_actions(num_actions, states, sess)\n",
    "\n",
    "            state, reward, episode_over = environment.next(actions[0])\n",
    "            states = np.array([state])\n",
    "            rewards[0] += reward\n",
    "            episodes_over[0] = episode_over\n",
    "\n",
    "        print('Performed {} tests for breakout.'.format(test_count))\n",
    "        print('Mean: {0:.2f}'.format(np.mean(rewards)))\n",
    "        print('Min: {0:.2f}'.format(np.min(rewards)))\n",
    "        print('Max: {0:.2f}'.format(np.max(rewards)))\n",
    "        print('Std: {0:.2f}'.format(np.std(rewards)))\n",
    "\n",
    "run()\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<img src=\"breakout.gif\">')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
